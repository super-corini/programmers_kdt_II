{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('pytlesson': conda)",
      "metadata": {
        "interpreter": {
          "hash": "f9e7add6a9c3ae5fa42f8e758fdc96e699e2f1248063b5a470cd9397e2825b39"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "2일차 실습: 애플 주식 데이터 처리하기",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYwHJBmiz6e5"
      },
      "source": [
        "애플 주식 데이터를 가지고 간단한 데이터 분석을 해보자. 모든 답은 Pyspark을 통해 이뤄져야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE0VhL0g1no8"
      },
      "source": [
        "먼저 PySpark과 Py4J를 설치하자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXgIyS_F0Kar"
      },
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.0.1 in c:\\anaconda\\envs\\pytlesson\\lib\\site-packages (3.0.1)\nRequirement already satisfied: py4j==0.10.9 in c:\\anaconda\\envs\\pytlesson\\lib\\site-packages (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNwc3F_Az6e6"
      },
      "source": [
        "#### Spark Session 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RveyavjYz6e7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark Dataframe basic example\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DgR89Sz6e8"
      },
      "source": [
        "#### 애플 주식 CSV 파일 로딩하기: https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\n",
        "일단 pandas 데이터프레임으로 로딩해서 Spark 데이터프레임으로 변경한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FeKmU3Piz6e8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "apple_pandas_df = pd.read_csv(\"https://pyspark-test-sj.s3-us-west-2.amazonaws.com/appl_stock.csv\")\n",
        "apple_spark_df = spark.createDataFrame(apple_pandas_df)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3fbJ4Lz6e9"
      },
      "source": [
        "#### 1> 어떤 컬럼 이름들이 있는가?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b341_1Zfz6e9"
      },
      "source": [
        "apple_spark_df.columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXj2LCWuz6e_"
      },
      "source": [
        "#### 2> 스키마를 프린트해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQR5dwZjz6e_"
      },
      "source": [
        "apple_spark_df.printSchema()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- Date: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Volume: long (nullable = true)\n |-- Adj Close: double (nullable = true)\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFljhmp5z6fA"
      },
      "source": [
        "#### 3> 처음 5개의 레코드를 출력해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQZ7PZDz6fA"
      },
      "source": [
        "apple_spark_df.show(n=5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+----------+---------+------------------+\n|      Date|      Open|      High|       Low|     Close|   Volume|         Adj Close|\n+----------+----------+----------+----------+----------+---------+------------------+\n|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|         27.727039|\n|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|         27.774976|\n|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178000000004|\n|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800|          27.28265|\n|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|         27.464034|\n+----------+----------+----------+----------+----------+---------+------------------+\nonly showing top 5 rows\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZPeXHxz6fB"
      },
      "source": [
        "#### 4> describe를 사용하여 데이터프레임의 컬럼별 통계보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Apv3ZhW2Zhj"
      },
      "source": [
        "apple_spark_df.describe().show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------+------------------+-----------------+-----------------+-------------------+------------------+\n|summary|      Date|              Open|              High|              Low|            Close|             Volume|         Adj Close|\n+-------+----------+------------------+------------------+-----------------+-----------------+-------------------+------------------+\n|  count|      1762|              1762|              1762|             1762|             1762|               1762|              1762|\n|   mean|      null|313.07631115891036| 315.9112880164586|309.8282405079455|312.9270656379115|9.422577587968218E7| 75.00174115607263|\n| stddev|      null|185.29946803981542|186.89817686485773|183.3839166437097|185.1471036170944|6.020518776592715E7|28.574929721799045|\n|    min|2010-01-04|              90.0|         90.699997|        89.470001|        90.279999|           11475900|         24.881912|\n|    max|2016-12-30|        702.409988|        705.070023|       699.569977|       702.100021|          470249500|        127.966091|\n+-------+----------+------------------+------------------+-----------------+-----------------+-------------------+------------------+\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR1QL8-Z2auU"
      },
      "source": [
        "#### 5> Close 컬럼의 평균값은 얼마인가?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCQCMa0xz6fB"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "apple_spark_df.select(mean(\"Close\")).show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n|       avg(Close)|\n+-----------------+\n|312.9270656379115|\n+-----------------+\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnE6Cbg_IONn"
      },
      "source": [
        "#### 6> Volume 컬럼의 최대값과 최소값은?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5mvFy0eIVPx"
      },
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "apple_spark_df.select(max(\"Volume\"), min(\"Volume\")).show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n|max(Volume)|min(Volume)|\n+-----------+-----------+\n|  470249500|   11475900|\n+-----------+-----------+\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax1Of8ATz6fD"
      },
      "source": [
        "#### 보너스 질문: HV ratio라는 이름의 새로운 컬럼을 추가한 데이터프레임을 만들기. 이 컬럼의 값은 High/Volume으로 계산된다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkO7rQ3Pz6fD"
      },
      "source": [
        "apple_spark_df_with_hv = apple_spark_df.withColumn(\"hv ratio\", apple_spark_df.High/apple_spark_df.Volume) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_bSkvOHEDC"
      },
      "source": [
        "apple_spark_df_with_hv.show(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+----------+---------+------------------+--------------------+\n|      Date|      Open|      High|       Low|     Close|   Volume|         Adj Close|            hv ratio|\n+----------+----------+----------+----------+----------+---------+------------------+--------------------+\n|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|         27.727039|1.737793286041590...|\n|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|         27.774976|1.432718223878593...|\n|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178000000004|1.559185743262822...|\n|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800|          27.28265|1.777288980473295...|\n|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|         27.464034|1.894503045949740...|\n+----------+----------+----------+----------+----------+---------+------------------+--------------------+\nonly showing top 5 rows\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIROQ8klz6fD"
      },
      "source": [
        "#### 보너스 질문: 월별 Close 컬럼의 평균값은?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_mr0fO_z6fD"
      },
      "source": [
        "from pyspark.sql.functions import month\n",
        "\n",
        "monthdf = apple_spark_df.withColumn(\"Month\", month(\"Date\"))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3mfqkYUHQ1e"
      },
      "source": [
        "monthavgdf = monthdf.select([\"Month\", \"Close\"]).groupBy(\"Month\").mean()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTpOy97YK_NC"
      },
      "source": [
        "monthavgdf.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------------------+\n|Month|avg(Month)|        avg(Close)|\n+-----+----------+------------------+\n|   12|      12.0| 302.3505362684563|\n|    1|       1.0| 322.2097142571429|\n|    6|       6.0|      288.12546566|\n|    3|       3.0| 332.9115673137254|\n|    5|       5.0| 351.6210208571428|\n|    9|       9.0|301.07631959027776|\n|    4|       4.0| 340.5104108150685|\n|    8|       8.0|300.43858096129026|\n|    7|       7.0|281.72216211486483|\n|   10|      10.0| 308.3055256315789|\n|   11|      11.0| 306.2725174895105|\n|    2|       2.0| 321.3595563037037|\n+-----+----------+------------------+\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mjKxisALA2t"
      },
      "source": [
        "monthavgdf.select([\"Month\", \"avg(Close)\"]).orderBy(\"Month\").show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n|Month|        avg(Close)|\n+-----+------------------+\n|    1| 322.2097142571429|\n|    2| 321.3595563037037|\n|    3| 332.9115673137254|\n|    4| 340.5104108150685|\n|    5| 351.6210208571428|\n|    6|      288.12546566|\n|    7|281.72216211486483|\n|    8|300.43858096129026|\n|    9|301.07631959027776|\n|   10| 308.3055256315789|\n|   11| 306.2725174895105|\n|   12| 302.3505362684563|\n+-----+------------------+\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egHUUucLToO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}